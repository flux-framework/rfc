ifdef::env-github[:outfilesuffix: .adoc]

14/Canonical Job Specification
==============================

A domain specific language based on YAML is defined to express the
resource requirements and other attributes of one or more programs
submitted to a Flux instance for execution.  This RFC describes the
canonical jobspec form, which represents a request to run exactly
one program.


* Name: github.com/flux-framework/rfc/spec_14.adoc
* Editor: Tom Scogland <scogland1@llnl.gov>
* State: raw

== Language

The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT", "SHOULD",
"SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL" in this document are to
be interpreted as described in http://tools.ietf.org/html/rfc2119[RFC 2119].

== Related Standards

* link:spec_4{outfilesuffix}[4/Flux Resource Model]
* link:spec_8{outfilesuffix}[8/Flux Task and Program Execution Services]

== Goals

* Express the resource requirements of a program to the scheduler.
* Allow graph-oriented resource requirements to be expressed.
* Express program attributes such as arguments, run time, and
task layout, to be considered by the program execution service (RFC 12)
* Express dependencies relative to other programs executing within
the same Flux instance.
* Emphasize expressivity over simplicity, as this canonical form
may be generated from other user-friendly forms or interfaces.
* Facilitate reproducible runs.
* Promote sharing and reuse of jobspec.

== Overview

This RFC describes the canonical form of "jobspec", a domain specific
language based on YAML
footnote:[http://yaml.org/spec/1.1/current.html[YAML Ain't Markup Language (YAML) Version 1.1], O. Ben-Kiki, C. Evans, B. Ingerson, 2004.]
.  The canonical jobspec SHALL consist of
a single YAML document representing a reusable request to run
exactly one program.  Hereafter, "jobspec" refers to the canonical
form, and "non-canonical jobspec" refers to the non-canonical form.

Non-canonical jobspec SHALL be decomposed into jobspec before
it is enqueued for the scheduler and program execution service.

User facing tools MAY generate jobspec from non-canonical jobspec,
or other sources.  Such tools MAY:

* generate a batch of dependent jobspecs representing a scientific workflow
* generate a stream of jobspecs representing a steered parameter study
* convert simulation parameters into jobspec containing computed
resource requirements, etc.
* convert command line arguments to jobspec, e.g. "flux mpirun"

=== Jobspec and Program Life Cycle

The jobspec SHALL be submitted to a job submission service.  Malformed
jobspec SHALL be immediately rejected by the job submission service.
A stack of plugins SHALL test jobspec against site or user defined
criteria, and on failure, MAY reject the jobspec, or MAY warn the user
and continue on.  The job submission service SHALL enqueue the jobspec
for consideration by the scheduler.

The scheduler SHALL consider each enqueued jobspec in the context of its
dependencies and the pool of available resources.  When the scheduler
chooses to execute a job, it allocates resources, associates them
with the jobspec, and notifies the program execution service to start
the program(s).

The program execution service, described in RFC 12, launches the program(s).
Task slots, containment, and task layout SHALL be created within the
allocated resources as described by the jobspec, or if that is not
possible, the job SHALL enter a failed state and resources SHALL
be returned to the scheduler.

Once a job is retired, the jobspec SHALL be retained as part of
its provenance record.

=== Resource Matching

Resources are represented as hierarchies or graphs, as described in RFC 4.

FIXME: describe how Flux hierarchical resource representation affects
jobspec design.

=== Terminology

FIXME: Fill in

== Jobspec Language Definition

A canonical jobspec YAML document SHALL consist of a dictionary
defining the resources, tasks and other attributes of a single
program. The dictionary MUST contain the keys `resources`, `tasks`,
`walltime`, `attrs`, and `version`.

Each of the listed jobspec keys SHALL meet the form and requirements
listed in detail in the sections below. For reference, a ruleset for
compliant canonical jobspec is provided using *JSON Content Rules*
footnoteref:[contentrules,https://tools.ietf.org/id/draft-newton-json-content-rules-06.txt[JSON Content Rules (jcr-version 0.6)]&#44; A. Newton&#44; P. Cordell&#44; 2016]
in the *Content Rules* at the end of this section.

=== Resources

The value of the `resources` key SHALL be a strict list which MUST
define at least one resource. Each list element SHALL represent a
*resource vertex*  or resource descriptor object as a dictionary
(described below).  The list of resources defined under the `resources`
key SHALL represent a composite resource request for the program
defined in the jobspec.

A resource vertex SHALL contain the following keys:

 *type*::
 The `type` key for a resource SHALL indicate the type of resource to
 be matched. Some type names MAY be reserved for use in the jobspec
 language itself. Currently reserved types include `slot`, used to
 define *task slots*, and `group` used to add an unnamed level of
 grouping to a resource request. Reserved types are described in the
 *Reserved Resource Types* section below.

 *count*::
 The `count` key SHALL indicate the desired number or range of
 resources matching the current vertex. The value of `count` SHALL be a
 dictionary which SHALL contain the following keys:
+
[horizontal]
   *min*::: The minimum required count or amount of this resource

   *max*::: The maximum required count or amount of this resource

   *operator*::: An operator applied between `min` and `max` which
   returns the next acceptable value

   *operand*::: The operand used in conjunction with `operator`
   to get the next value between `min` and `max`.

A resource vertex MAY additionally contain one or more of the
following keys

 *unit*::
 The `unit` key, if supplied, SHALL have a string value indicating
 the chosen units applied to the `count` value or values.

 *exclusive*::
 The `exclusive` key SHALL be a boolean  indicating, when true,  that
 the current resource is requested to be allocated exclusively to
 the current program. If unset, the default value for `exclusive` SHALL
 be `false` for vertices that are not within a task slot. The default
 value for `exclusive` SHALL be `true` for task slots (`type: slot`)
 and their associated resources.

 *with*::
 The `with` key SHALL indicate an edge of type `out` from this resource
 vertex to another resource. Therefore, the value of the `with` key
 SHALL be a dictionary conforming to the resource vertex specification.

 *edge*::
 **XXX**: need specification for other "edge match descriptors"

 *id*::
 The value of the `id` key SHALL be a string indicating a set of
 matching resource identifiers.

 *uuid*::
 The value of the `uuid` key SHALL be a string indicating a set of
 matching resource UUIDs.

 *tags*::
 The value of the `tags` key SHALL be a dictionary with supported key
 `list` which SHALL be a list of string tags required. Other keys MAY
 be reserved for future or site-specific extensions.

 *attributes*::
 The value of the `attributes` key SHALL be a dictionary with supported
 key `list` which SHALL be a list of dictionaries with required attributes
 as keys, and their required values as the key values. Other keys in
 the attributes dictionary MAY be reserved for future or site-specific
 extensions.


==== Reserved Resource Types

*group*:: A resource type of `type: group` SHALL indicate an anonymous
grouping of resources in the resource request.

*slot*:: A resource type of `type: slot` SHALL indicate a grouping
of resources into a named *task slot*. A `slot` SHALL be a valid
resource spec including a `label` key, the value of which may be used
to reference the named task slot during tasks definition. The `label`
provided SHALL be local to the namespace of the current jobspec.
+
A task slot SHALL have at least one edge specified using `with:`, and
the resources associated with a slot SHALL be exclusively allocated
to the program described in the jobspec.

=== Tasks

The value of the `tasks` key SHALL be a strict list which MUST
define at least one task. Each list element SHALL be a dictionary
representing a task or tasks to run as part of the program. A task
descriptor SHALL contain the following keys:

 *command*::
 The value of the `command` key SHALL be a string OR list representing
 an executable and its arguments.

 *slot*::
 The value of the `slot` key SHALL be used to indicate the *task slot*
 on which this task or tasks shall be contained and executed. The
 number of tasks executed per task slot SHALL be a function of the
 number of resource slots and total number of tasks requested to execute.
+
The value of the `slot` key SHALL be a dictionary with supported key
of either `label` or `level`. The `label` key SHALL reference a `label`
from the `resources` list, indicating an explicitly created and named
*task slot*. The `level` key SHALL reference a resource name, such
as `core` or `node`, indicating an implicitly created *task slot* on
which to map the defined tasks.

 *count*::
 The value of the `count` key SHALL be a dictionary supporting at least
 the keys `per_slot` and `total`, with other keys reserved for future
 or site-specific extensions.
+
[horizontal]
  *per_slot*:::
  The value of `per_slot` SHALL be a number indicating the number
  of tasks to execute per task slot allocated to the program.

  *total*:::
  The value of the `total` field SHALL indicate the total number of
  tasks to be run across all task slots, possibly oversubscribed.

 *attrs*::
 The `attrs` key SHALL be a free-form dictionary of keys which may
 be used for platform independent or optional extensions.

 *distribution*::
 The value of the `distribution` key SHALL be a string, which MAY
 be used as input to the launcher's algorithm for task placement and
 layout among task slots.

=== Walltime
The value of the `walltime` key SHALL be a dictionary with currently
supported keys `duration` or `range`. Other keys MAY be reserved for
future or site specific extensions. It SHALL be considered an error
if multiple, conflicting keys are set in the `walltime` dictionary.

Supported keys for `walltime` are described in detail below.

 *duration*:: The `duration` key SHALL have a string value interpreted
 as a single duration walltime estimate/limit for the program described
 by the current jobspec.

 *range*:: The `range` key SHALL be a dictionary with duration keys
 `min` and `max` indicating the minimum and maximum walltime
 estimate/limit for the program in the current jobspec.

=== Attributes
The value of the `attrs` key SHALL be a dictionary of generic
program attributes, system or user defined, which MAY affect program
execution, scheduling, task placement, etc. Some key names in the
`attrs` dictionary MAY be reserved.


=== Example Jobspec

Under the description above, the following is an example of a fully compliant
version 1 jobspec. The example below declares a request for 4 "nodes"
each of which with 1 task slot consisting of 2 cores each, for a total
of 4 task slots. A single copy of the command `app` will be run on each
task slot for a total of 4 tasks.

[source,yaml]
----
version: 1
resources:
  - type: node
    count:
      min: 4
      max: 4
      operator: "+"
      operand: 1
    with:
      - type: slot
        count:
          min: 1
          max: 1
          operator: "+"
          operand: 1
        label: default
        with:
          - type: core
            count:
              min: 2
              max: 2
              operator: "+"
              operand: 1
tasks:
  - command: app
    slot:
      label: default
    count:
      per_slot: 1
walltime:
  duration: 1h
attrs:
----

A simpler example using implicit *task slot* definition to run 4 tasks
across 4 nodes

[source,yaml]
----
version: 1
resources:
  - type: node
    count:
      min: 4
      max: 4
      operator: "+"
      operand: 1
tasks:
  - command: hostname
    slot:
      level: node
    count:
      per_slot: 1
walltime:
  duration: 1h
attrs:
----

=== Content Rules

A jobspec conforming to version 1 of the language definition SHALL
adhere to the following ruleset, described using JSON Content Rules footnoteref:[contentrules]
draft version 0.6.

----
# jcr-version 0.6

{
   "resources" : [ +vertex ],
   "tasks" : tasks,
   "walltime" : walltime,
   "attrs" :  { /.*/ : any },
   "version" : 1,
}

vertex_common {
    "count" : resource_count,
    ?"exclusive" : boolean,
    ?"with" : [ +vertex ],
    ?"edge" : TBD,
}

group_vertex {
    "type" : "group",
    vertex_common,
}

slot_vertex {
    "type"  : "slot",
    "label" : string,
    vertex_common,
}

resource_vertex {
    "type" : ( :string, + @{reject} ( "group", "slot")),
    vertex_common
    ?"id" : string,
    ?"uuid" : string,
    ?"tags" : resource_tags,
    ?"attributes" : resource_attributes,
    ?"unit" : string,
}

vertex ( group_vertex | slot_vertex | resource_vertex )

resource_count {
    "min" 1..,
    "max" 1..,
    "operator" : ( :"+" | :"*" | : "^" ),
    "operand" : 1..,
}

resource_tags {
    ?"list" : [ +string ],
}

resource_attributes {
    ?"list" : TBD,
}

tasks {
    "command" : [ +string ],
    "slot" : { "label" : string  | "level": string },
    "count" : { "per_slot" : 1.. | "total" : 1.. },
    "distribution" : string,
    ?"attrs" : { /.*/ : any },
}

walltime {
    "duration" : string |
    "range" { "min" : 0.., "max" : 0.. },
}

----



== Basic Use Cases

To implement basic resource manager functionality, the following use
cases SHALL be supported by the jobspec:

=== Section 1: Resource only requests

The following "resource only" requests are assumed to be the equivalent
of existing resource manager batch job submission or allocation
requests, i.e. equivalent to `oarsub`, `qsub`, and `salloc`. In terms
of a canonical jobspec, these requests are assumed to be requests
to start an instance, i.e. run a single copy of `flux start` per
allocated node.

'''
Use Case 1.1:: Request Single Resource with Count
+
Specific Example:: Request 4 nodes
+
Existing Equivalents::
+
|===
| Slurm | `salloc -N4`
| PBS | `qsub -l nodes=4`
|===
+
Jobspec YAML::
+
[source,yaml]
version: 1
walltime: 1h
resources:
  - type: node
    count:
      min: 4
      max: 4
      operator: "+"
      operand: 1
tasks:
  - command: [ "flux", "start" ]
    slot:
      level: node
    count:
      per_slot: 1
attrs:

'''
Use Case 1.2:: Request a range of a type of resource
+
Specific Example:: Request between 3 and 30 nodes
+
Existing Equivalents::
+
|===
| Slurm | `salloc -N3-30`
|===
+
Jobspec YAML::
+
[source,yaml]
version: 1
walltime: 1h
resources:
  - type: node
    count:
      min: 3
      max: 30
      operator: "+"
      operand: 1
tasks:
  - command: [ "flux", "start" ]
    slot:
      level: node
    count:
      per_slot: 1
attrs:

'''
Use Case 1.3:: Request M nodes with a minimum number of sockets per node
and cores per socket
+
Specific Example:: Request 4 nodes with at least 2 sockets each,
and 4 cores per socket
+
Existing Equivalents::
+
|===
| Slurm (a)| `srun -N4 --sockets-per-node=2 --cores-per-socket=4`
| Slurm (b)| `srun -N4 -B '2:4:*'`
| OAR      | `oarsub -l nodes=4/sockets=2/cores=4`
|=== 
+
Jobspec YAML::
+
[source,yaml]
version: 1
walltime: 1h
resources:
  - type: node
    count:
      min: 4
      max: 4
      operator: "+"
      operand: 1
    with:
      - type: socket
        count:
          min: 2
          max: 2
          operator: "+"
          operand: 1
        with:
          - type: core
            count:
              min: 4
              max: 4
              operator: "+"
              operand: 1
tasks:
  - command: [ "flux", "start" ]
    slot:
      level: node
    count:
      per_slot: 1
attrs:

'''
Use Case 1.4:: Exclusively allocate nodes, while constraining cores and
sockets.
+
Specific Example:: Request an *exclusive* allocation of 4 nodes that have at
least two sockets and 4 cores per socket:
+
Jobspec YAML::
+
[source,yaml]
version: 1
walltime: 1h
resources:
  - type: slot
    with:
    - type: node
      count:
        min: 4
        max: 4
        operator: "+"
        operand: 1
      with:
        - type: socket
          count:
            min: 2
            max: 2
            operator: "+"
            operand: 1
          with:
            - type: core
              count:
                min: 4
                max: 4
                operator: "+"
                operand: 1
tasks:
  - command: [ "flux", "start" ]
    slot:
      level: node
    count:
      per_slot: 1
attrs:

'''
Use Case 1.5:: Complex example from OAR
+
Specific Example::
+
[quote, http://oar.imag.fr/docs/2.5/user/usecases.html#mixing-every-together]
ask for 1 core on 2 nodes on the same cluster with 4096 GB of memory
and Infiniband 10G + 1 cpu on 2 nodes on the same switch with bicore
processors for a walltime of 4 hours
+
Existing Equivalents::
+
|===
| OAR | `oarsub -I -l "{memnode=4096 and ib10g='YES'}/cluster=1/nodes=2/core=1+{nbcore=2}/switch=1/nodes=2/cpu=1,walltime=4:0:0"`
|===
+
Jobspec YAML::
+
[source,yaml]
version: 1
resources:
  - type: cluster
    count:
      min: 1
      max: 1
      operator: "+"
      operand: 1
    with:
      - type: node
        count:
          min: 2
          max: 2
          operator: "+"
          operand: 1
        with:
          - type: memory
            count:
              min: 4
              max: 4
              operator: "+"
              operand: 1
            unit: GB
          - type: ib10g
            count:
              min: 1
              max: 1
              operator: "+"
              operand: 1
      - type: switch
        with:
          type: node
            count:
              min: 2
              max: 2
              operator: "+"
              operand: 1
            with:
                - type: core
                  count:
                    min: 1
                    max: 1
                    operator: "+"
                    operand: 1
walltime: 4h
tasks:
  - command: [ "flux", "start" ]
    slot:
      level: node
    count:
      per_slot: 1
attrs:

'''
Use Case 1.6:: Request resources across multiple clusters
+
Specific Example::
Ask for 1 core on 15 nodes across 2 clusters (total = 30 cores)
+
Existing Equivalents::
+
|===
| OAR |  `oarsub -I -l /cluster=2/nodes=15/core=1`
|===
+
Jobspec YAML::
+
[source,yaml]
version: 1
walltime: 1h
resources:
    - type: cluster
      count:
        min: 2
        max: 2
        operator: "+"
        operand: 1
      with:
          - type: node
            count:
              min: 15
              max: 15
              operator: "+"
              operand: 1
            with:
              - type: core
                count:
                  min: 1
                  max: 1
                  operator: "+"
                  operand: 1
tasks:
  - command: [ "flux", "start" ]
    slot:
      level: node
    count:
      per_slot: 1
attrs:

'''
Use Case 1.7:: Request N cores across M switches
+
Specific Example::
Request 3 cores across 3 switches
+
Existing Equivalents::
+
|===
| OAR | `oarsub -I -l /switch=3/core=1`
|===
+
Jobspec YAML::
+
[source,yaml]
version: 1
walltime: 1h
resources:
  - type: switch
    count:
      min: 3
      max: 3
      operator: "+"
      operand: 1
    with:
      - type: core
        count:
          min: 1
          max: 1
          operator: "+"
          operand: 1
tasks:
  - command: [ "flux", "start" ]
    slot:
      level: node
    count:
      per_slot: 1
attrs:

'''

=== Section 2: Resource and task jobspec

The following use cases include task specification in addition to resource
request, demonstrating the use of task slot *labels* and *levels* to relate
tasks to resources.

'''
Use Case 2.1:: Run N tasks across M nodes
+
Specific Example:: Run `hostname` 20 times on 4 nodes, 5 per node
+
Existing Equivalents::
+
|===
| Slurm | `srun -N4 -n20 hostname` or `srun -N4 --ntasks-per-node=5 hostname`
| PBS   | `qsub -l nodes=4,mppnppn=5`
|===
+
Jobspec YAML::
+
[source,yaml]
version: 1
walltime: 1h
resources:
  - type: slot
    label: default
    count:
      min: 4
      max: 4
      operator: "+"
      operand: 1
    with:
    - type: node
      count:
        min: 1
        max: 1
        operator: "+"
        operand: 1
tasks:
  - command: hostname
    slot: default
    count:
      per_slot: 5
attrs:

'''
Use Case 2.2:: Run N tasks across M nodes, unequal distribution
+
Specific Example:: Run 5 copies of `hostname` across 4 nodes,
default distribution
+
Existing Equivalents::
+
|===
| Slurm | `srun -n5 -N4 hostname`
|===
+
Jobspec YAML::
+
[source,yaml]
----
version: 1
walltime: 1h
resources:
  - type: node
    count:
      min: 4
      max: 4
      operator: "+"
      operand: 1
tasks:
  - command: hostname
    slot:
      level: node
    count:
      total: 5
attrs:
----
+
or, with explicit task slots:
+
[source,yaml]
----
version: 1
walltime: 1h
resources:
  - type: slot
    count:
      min: 4
      max: 4
      operator: "+"
      operand: 1
    label: myslot
    with:
      - type: node
        count:
          min: 1
          max: 1
          operator: "+"
          operand: 1
tasks:
  - command: hostname
    slot:
      label: myslot
    count:
      total: 5
attrs:
----

'''
Use Case 2.3:: Run N tasks, Require M cores per task
+
Specific Example:: Run 10 copies of `myapp`, require 2 cores per copy,
for a total of 20 cores
+
Existing Equivalents::
+
|===
| Slurm | `srun -n10 -c 2 myapp`
|===
+
Jobspec YAML::
+
[source,yaml]
version: 1
walltime: 1h
resources:
  - type: slot
    label: default
    count:
      min: 10
      max: 10
      operator: "+"
      operand: 1
    with:
      - type: core
        count:
          min: 2
          max: 2
          operator: "+"
          operand: 1
tasks:
  - command: myapp
    slot: default
    count:
      per_slot: 1
attrs:

'''
Use Case 2.4:: Run different binaries with differing resource
requirements as single program
+
Specific Example:: 11 tasks, one node, first 10 using one core and 4G of RAM for
`read-db`, last using 6 cores and 24G of RAM for `db`
+
Existing Equivalents:: None Known
+
Jobspec YAML::
+
[source,yaml]
version: 1
walltime: 1h
resources:
  - type: node
    with:
      - type: slot
        label: read-db
        count:
          min: 10
          max: 10
          operator: "+"
          operand: 1
        with:
          - type: core
            count:
              min: 1
              max: 1
              operator: "+"
              operand: 1
          - type: memory
            count:
              min: 4
              max: 4
              operator: "+"
              operand: 1
            unit: GB
      - type: slot
        label: db
        count:
          min: 1
          max: 1
          operator: "+"
          operand: 1
        with:
          - type: core
            count:
              min: 6
              max: 6
              operator: "+"
              operand: 1
          - type: memory
            count:
              min: 24
              max: 24
              operator: "+"
              operand: 1
            unit: GB
tasks:
  - command: read-db
    slot: read-db
    count:
      per_slot: 1
  - command: db
    slot: db
    count:
      per_slot: 1
attrs:

'''
Use Case 2.5:: Run command requesting minimum amount of RAM per core
+
Specific Example::
Run 10 copies of `app` across 10 cores with at least 2GB per core
+
Existing Equivalents::
+
|===
| Slurm | `srun -n 10 --mem-per-cpu=2048 app`
|===
+
Jobspec YAML::
+
[source,yaml]
----
version: 1
walltime: 1h
resources:
  - type: slot
    label: default
    count:
      min: 10
      max: 10
      operator: "+"
      operand: 1
    with:
    - type: memory
      count:
        min: 2
        max: 2
        operator: "+"
        operand: 1
      unit: GB
    - type: core
      count:
        min: 1
        max: 1
        operator: "+"
        operand: 1
tasks:
  - command: app
    slot: default
    count:
      per_slot: 1
attrs:
----
'''
Use Case 2.6:: Run N copies of a command with minimum amount of RAM per node
+
Specific Example::
Run 10 copies of `app` across 2 nodes with at least 4GB per node
+
Existing Equivalents::
+
|===
| Slurm | `srun -n10 -N2 --mem=4096 app`
| OAR   | `oarsub -p memnode=4096 -l nodes=2 "taktuk -c oarsh -f $OAR_FILE_NODES broadcast exec [app]"`
|===
+
Jobspec YAML::
+
[source,yaml]
version: 1
walltime: 1h
resources:
  - type: slot
    label: 4GB-node
    count:
      min: 2
      max: 2
      operator: "+"
      operand: 1
    with:
    - type: node
      count:
        min: 1
        max: 1
        operator: "+"
        operand: 1
      with:
        - type: memory
          count:
            min: 4
            max: 4
            operator: "+"
            operand: 1
          unit: GB
tasks:
  - command: app
    slot: 4GB-node
    count:
      total: 10
attrs:

[sect2]
== References

* https://www.ogf.org/documents/GFD.56.pdf[Job Submission Description Language (JSDL) Specification, Version 1.0, Ali Anjomshoaa, et al., 2005]
